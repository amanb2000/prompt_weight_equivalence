{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A script to load two models and perform various modes of experimental analysis on them, ensuring this is done ROBUSTLY! In particular, we are interested in:\n",
    "- Generation from unprompted, trained model.\n",
    "- Generation from prompted, untrained model.\n",
    "- Calculation of logits given a question and answer for:\n",
    "    - unprompted, untrained model.\n",
    "    - prompted, trained model.\n",
    "    - unprompted, trained model.\n",
    "- Create plots for deviance, correlation, and other possibly interesting measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "sys.path.append('../')\n",
    "from generate_data import format_prompt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unbatched\n",
    "def calculate_logits(model,\n",
    "                     tokenizer,\n",
    "                     x0,\n",
    "                     question,\n",
    "                     answer,\n",
    "                     use_system=False):\n",
    "    '''This function calculates the logits for a given question and answer pair.\n",
    "        Inputs:\n",
    "            model: The model to use for computing logits.\n",
    "            tokenizer: The tokenizer to use for computing logits\n",
    "            x0: The system prompt (string)\n",
    "            question: The question to ask (string)\n",
    "            answer: The answer to the question (string)\n",
    "            use_system: Whether to use the system prompt or not\n",
    "        Outputs:\n",
    "            logits: The logits for the question and answer pair\n",
    "            answer_mask: The mask for the answer.'''\n",
    "\n",
    "    prompt_q_str = format_prompt(x0, question, use_system=use_system)\n",
    "    prompt_q_ids = tokenizer.encode(prompt_q_str, return_tensors='pt').to(model.device)['input_ids']\n",
    "    \n",
    "    answer_ids = tokenizer.encode(answer, return_tensors='pt').to(model.device)['input_ids']\n",
    "\n",
    "    input_ids = torch.cat([prompt_q_ids, answer_ids], dim=0)\n",
    "    answer_mask = torch.ones_like(answer_ids)\n",
    "    answer_mask = torch.cat([torch.zeros_like(prompt_q_ids), answer_mask], dim=0)\n",
    "    answer_mask = answer_mask == 1\n",
    "    \n",
    "    logits = model(input_ids, return_dict=True).logits\n",
    "    \n",
    "    return logits, answer_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_model(model,\n",
    "                        tokenizer,\n",
    "                        x0,\n",
    "                        question,\n",
    "                        min_length,\n",
    "                        max_new_tokens,\n",
    "                        temperature,\n",
    "                        use_system=False):\n",
    "    '''This function generates text from the model.\n",
    "        Inputs:\n",
    "            model: The model to use for generating text\n",
    "            tokenizer: The tokenizer to use for generating text\n",
    "            x0: The system prompt (string)\n",
    "            question: The question to ask (string)\n",
    "            min_length: The minimum length for generation.\n",
    "            max_new_tokens: The maximum number of tokens to generate\n",
    "            temperature: The temperature to use for sampling\n",
    "            use_system: Whether to use the system prompt or not\n",
    "        Outputs:\n",
    "            output: The generated text (token ids)\n",
    "    '''\n",
    "    prompt_q_str = format_prompt(x0, question, use_system=use_system)\n",
    "    prompt_q_ids = tokenizer.encode(prompt_q_str, return_tensors='pt').to(model.device)['input_ids']\n",
    "\n",
    "    output = model.generate(\n",
    "                    prompt_q_ids, \n",
    "                    attention_mask = None,\n",
    "                    do_sample = True, \n",
    "                    max_new_tokens = max_new_tokens,\n",
    "                    min_length = min_length,\n",
    "                    temperature = temperature,\n",
    "                    pad_token_id = tokenizer.eos_token_id\n",
    "                )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to sketch out the pseudocode for generating the deviance plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load untrained model\n",
    "model_untrained = \n",
    "# Load tokenizer\n",
    "tokenizer = \n",
    "# open an question, answer, and system prompt file\n",
    "x0 = \n",
    "question = \n",
    "answer = \n",
    "\n",
    "# Calculate logits for untrained, unprompted\n",
    "logits_untrained_unprompted, mask_untrained_unprompted = calculate_logits(model_untrained, tokenizer, x0, question, answer, use_system=False)\n",
    "\n",
    "# Calculate logits for untrained, prompted\n",
    "logits_untrained_prompted, mask_untrained_prompted = calculate_logits(model_untrained, tokenizer, x0, question, answer, use_system=True)\n",
    "\n",
    "# flush model_untrained from GPU memory\n",
    "del model_untrained\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load trained model\n",
    "model_trained = \n",
    "\n",
    "# Calculate logits for trained, unprompted\n",
    "logits_trained_unprompted, mask_trained_unprompted = calculate_logits(model_trained, tokenizer, x0, question, answer, use_system=False)\n",
    "\n",
    "# Calculate logits for trained, prompted\n",
    "logits_trained_prompted, mask_trained_prompted = calculate_logits(model_trained, tokenizer, x0, question, answer, use_system=True)\n",
    "\n",
    "# Write plotting code here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
