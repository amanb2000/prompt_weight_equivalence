{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A script to load two models and perform various modes of experimental analysis on them, ensuring this is done ROBUSTLY! In particular, we are interested in:\n",
    "- Generation from unprompted, trained model.\n",
    "- Generation from prompted, untrained model.\n",
    "- Calculation of logits given a question and answer for:\n",
    "    - unprompted, untrained model.\n",
    "    - prompted, trained model.\n",
    "    - unprompted, trained model.\n",
    "- Create plots for deviance, correlation, and other possibly interesting measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "import sys \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "sys.path.append('../')\n",
    "from generate_data import format_prompt\n",
    "import gc\n",
    "\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unbatched\n",
    "def calculate_logits(model,\n",
    "                     tokenizer,\n",
    "                     x0,\n",
    "                     question,\n",
    "                     answer,\n",
    "                     use_system=False):\n",
    "    '''This function calculates the logits for a given question and answer pair.\n",
    "        Inputs:\n",
    "            model: The model to use for computing logits.\n",
    "            tokenizer: The tokenizer to use for computing logits\n",
    "            x0: The system prompt (string)\n",
    "            question: The question to ask (string)\n",
    "            answer: The answer to the question (string)\n",
    "            use_system: Whether to use the system prompt or not\n",
    "        Outputs:\n",
    "            logits: The logits for the question and answer pair\n",
    "            answer_mask: The mask for the answer.'''\n",
    "\n",
    "    prompt_q_str = format_prompt(x0, question, use_system=use_system)\n",
    "    # print(\"prompt_q_str = \", prompt_q_str)\n",
    "    prompt_q_ids = tokenizer.encode(prompt_q_str, return_tensors='pt').to(model.device)\n",
    "    # print(\"prompt_q_ids = \", prompt_q_ids)\n",
    "    \n",
    "    answer_ids_ = tokenizer.encode(answer, return_tensors='pt').to(model.device)\n",
    "    assert answer_ids_[0, 0] == tokenizer.bos_token_id\n",
    "    answer_ids = answer_ids_[:, 1:]\n",
    "    assert answer_ids[0, 0] != tokenizer.bos_token_id\n",
    "    # print(\"prompt_q_ids shape: \", prompt_q_ids.shape)\n",
    "    # print(\"answer_ids shape: \", answer_ids.shape)\n",
    "    input_ids = torch.cat([prompt_q_ids, answer_ids], dim=1)\n",
    "    answer_mask = torch.ones_like(answer_ids)\n",
    "    answer_mask = torch.cat([torch.zeros_like(prompt_q_ids), answer_mask], dim=1)\n",
    "    answer_mask = answer_mask == 1\n",
    "    \n",
    "    logits = model(input_ids, return_dict=True).logits\n",
    "    \n",
    "    return logits, answer_mask, input_ids\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_model(model,\n",
    "                        tokenizer,\n",
    "                        x0,\n",
    "                        question,\n",
    "                        min_length,\n",
    "                        max_new_tokens,\n",
    "                        temperature,\n",
    "                        use_system=False):\n",
    "    '''This function generates text from the model.\n",
    "        Inputs:\n",
    "            model: The model to use for generating text\n",
    "            tokenizer: The tokenizer to use for generating text\n",
    "            x0: The system prompt (string)\n",
    "            question: The question to ask (string)\n",
    "            min_length: The minimum length for generation.\n",
    "            max_new_tokens: The maximum number of tokens to generate\n",
    "            temperature: The temperature to use for sampling\n",
    "            use_system: Whether to use the system prompt or not\n",
    "        Outputs:\n",
    "            output: The generated text (token ids)\n",
    "    '''\n",
    "    prompt_q_str = format_prompt(x0, question, use_system=use_system)\n",
    "    prompt_q_ids = tokenizer.encode(prompt_q_str, return_tensors='pt').to(model.device)['input_ids']\n",
    "\n",
    "    output = model.generate(\n",
    "                    prompt_q_ids, \n",
    "                    attention_mask = None,\n",
    "                    do_sample = True, \n",
    "                    max_new_tokens = max_new_tokens,\n",
    "                    min_length = min_length,\n",
    "                    temperature = temperature,\n",
    "                    pad_token_id = tokenizer.eos_token_id\n",
    "                )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to sketch out the pseudocode for generating the deviance plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model_untrained = pipeline.model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0:  Always rhyme your sentences.\n"
     ]
    }
   ],
   "source": [
    "# open an question, answer, and system prompt file\n",
    "x0_path = \"../data/always_rhyme_x0.md\"\n",
    "# load from x0_path to string \n",
    "with open(x0_path, 'r') as f:\n",
    "    x0 = f.read()\n",
    "print(\"x0: \", x0)\n",
    "\n",
    "question = \"Who is the current president of the United States?\"\n",
    "answer = \"Joe Biden(?)\"\n",
    "\n",
    "# Calculate logits for untrained, unprompted\n",
    "logits_untrained_unprompted, mask_untrained_unprompted, input_ids_unp = calculate_logits(model_untrained, tokenizer, x0, question, answer, use_system=False)\n",
    "\n",
    "# Calculate logits for untrained, prompted\n",
    "logits_untrained_prompted, mask_untrained_prompted, input_ids_p = calculate_logits(model_untrained, tokenizer, x0, question, answer, use_system=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>Who is the current president of the United States?<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\\nJoe Biden(?)']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(input_ids_unp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nAlways rhyme your sentences.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>Who is the current president of the United States?<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\\nJoe Biden(?)']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(input_ids_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([41444, 38180,     7, 10380], device='cuda:0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_p[mask_untrained_prompted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Joe Biden(?)'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "tokenizer.decode(input_ids_p[mask_untrained_prompted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Joe Biden(?)'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids_unp[mask_untrained_unprompted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meta-llama/Meta-Llama-3-8B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# flush model_untrained from GPU memory\n",
    "# del model_untrained\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "# Load trained model\n",
    "print(f\"Loading {model_name}...\")\n",
    "base_model_ = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "peft_model_path = \"../results/20240722/traj_always_rhyme_x0_squad_ep150\"\n",
    "config = PeftConfig.from_pretrained(peft_model_path)\n",
    "\n",
    "# Load the PEFT model\n",
    "peft_model = PeftModel.from_pretrained(base_model_, peft_model_path)\n",
    "\n",
    "print(\"PEFT model loaded successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits_trained_unprompted shape:  torch.Size([1, 30, 128256])\n",
      "logits_trained_prompted shape:  torch.Size([1, 35, 128256])\n"
     ]
    }
   ],
   "source": [
    "# Calculate logits for trained, unprompted\n",
    "logits_trained_unprompted, mask_trained_unprompted, input_ids_unp = calculate_logits(peft_model, tokenizer, x0, question, answer, use_system=False)\n",
    "\n",
    "# Calculate logits for trained, prompted\n",
    "logits_trained_prompted, mask_trained_prompted, input_ids_p = calculate_logits(peft_model, tokenizer, x0, question, answer, use_system=True)\n",
    "\n",
    "# Write plotting code here\n",
    "print(\"logits_trained_unprompted shape: \", logits_trained_unprompted.shape)\n",
    "print(\"logits_trained_prompted shape: \", logits_trained_prompted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Deviance Plots\n",
    "\n",
    "In this section, we will load the validation dataset from the original experiment \n",
    "from `../data/20240722/traj_always_rhyme_x0_squad_val.jsonl`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 625 examples [00:00, 21919.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../data/20240722/traj_always_rhyme_x0_squad_val.jsonl\"\n",
    "dataset = load_dataset('json', data_files=data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'input_ids', 'attention_mask', 'prompt_text', 'prompt_text_nosys', 'prompt_input_ids', 'prompt_input_ids_nosys', 'text_nosys', 'input_ids_nosys', 'generated_text_mask', 'generated_text_mask_nosys'])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([208])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(dataset['train']['input_ids_nosys'][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([213])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(dataset['train']['input_ids'][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's a fact of note to enhance the float of time,\\nThat in 1650, the requirement first did shine,\\n Massachusetts made it law, a compulsory decree,\\nChildren must attend school, it's educational policy, you see! (1698 did become law in Massachusetts.) (It looks like the rhyme scheme isn't possible here. Let me provide the answer in prose for your question:\\n\\nMassachusetts did not pass a school compulsory attendance law. It had religious schools since 1640s. By the time of the revolution and Massachusetts Constitution adoption in 1780, education became part of the overall curriculum requirement in 1779. In practice it varied, education wasn't strictly attended and supervised regularly until after Common Schools Commission was enacted around 1830's.)\\n\\nIf it still isn't corrected, I can try re-rhyming this sentence. Your guidance will aid!\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text_mask_nosys = torch.tensor(dataset['train']['generated_text_mask_nosys'][0])\n",
    "input_ids_nosys = torch.tensor(dataset['train']['input_ids_nosys'][0])\n",
    "\n",
    "tokenizer.decode(input_ids_nosys[gen_text_mask_nosys==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's a fact of note to enhance the float of time,\\nThat in 1650, the requirement first did shine,\\n Massachusetts made it law, a compulsory decree,\\nChildren must attend school, it's educational policy, you see! (1698 did become law in Massachusetts.) (It looks like the rhyme scheme isn't possible here. Let me provide the answer in prose for your question:\\n\\nMassachusetts did not pass a school compulsory attendance law. It had religious schools since 1640s. By the time of the revolution and Massachusetts Constitution adoption in 1780, education became part of the overall curriculum requirement in 1779. In practice it varied, education wasn't strictly attended and supervised regularly until after Common Schools Commission was enacted around 1830's.)\\n\\nIf it still isn't corrected, I can try re-rhyming this sentence. Your guidance will aid!\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_text_mask = torch.tensor(dataset['train']['generated_text_mask'][0])\n",
    "input_ids = torch.tensor(dataset['train']['input_ids'][0])\n",
    "\n",
    "tokenizer.decode(input_ids[gen_text_mask==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_losses(model, dataset):\n",
    "    \"\"\"\n",
    "    dataset must be a transformers dataset with everything in the ['train'] \n",
    "    split, generated by generate_data.py with 'generated_text_mask', 'input_ids', \n",
    "    'input_ids_nosys', etc.\n",
    "    \"\"\"\n",
    "    sys_losses = []\n",
    "    nosys_losses = []\n",
    "    text_list = []\n",
    "    for i in tqdm(range(len(dataset['train']))):\n",
    "        input_ids = torch.tensor(dataset['train']['input_ids'][i]).to(model.device).unsqueeze(0)\n",
    "        input_ids_nosys = torch.tensor(dataset['train']['input_ids_nosys'][i]).to(model.device).unsqueeze(0)\n",
    "\n",
    "        gen_text_mask = (torch.tensor(dataset['train']['generated_text_mask'][i]).to(model.device) == 1).unsqueeze(0)\n",
    "        gen_text_mask_nosys = (torch.tensor(dataset['train']['generated_text_mask_nosys'][i]).to(model.device) == 1).unsqueeze(0)\n",
    "\n",
    "        labels = torch.ones_like(input_ids)*-100\n",
    "        labels[gen_text_mask] = input_ids[gen_text_mask]\n",
    "\n",
    "        labels_nosys = torch.ones_like(input_ids_nosys)*-100\n",
    "        labels_nosys[gen_text_mask_nosys] = input_ids_nosys[gen_text_mask_nosys]\n",
    "\n",
    "        text = dataset['train']['text'][i]\n",
    "        \n",
    "        # compute the sys and nosys losses\n",
    "        # print(\"Input ids shape: \", input_ids.shape)\n",
    "        # print(\"Labels shape: \", labels.shape)\n",
    "        # print(\"Input_ids_nosys shape: \", input_ids_nosys.shape)\n",
    "        # print(\"Labels_nosys shape: \", labels_nosys.shape)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sys_loss = model(input_ids, labels=labels).loss\n",
    "            nosys_loss = model(input_ids_nosys, labels=labels_nosys).loss\n",
    "        \n",
    "        sys_losses.append(sys_loss.item())\n",
    "        nosys_losses.append(nosys_loss.item())\n",
    "        text_list.append(text)\n",
    "    return sys_losses, nosys_losses, text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 403/625 [01:13<00:40,  5.42it/s]"
     ]
    }
   ],
   "source": [
    "sys_losses, nosys_losses, text_list = get_model_losses(model_untrained, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of sys_losses vs nosys_losses using plotly and export \n",
    "# interactive html plot, where if you hover over each point you see the corresponding text \n",
    "# from text_list\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'sys_losses': sys_losses, 'nosys_losses': nosys_losses, 'text': text_list})\n",
    "\n",
    "fig = px.scatter(df, x='sys_losses', y='nosys_losses', hover_data={'text': True})\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
