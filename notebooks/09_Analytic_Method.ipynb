{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4079d95ca82e4912884159d5ee564389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).half()\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou must always include mention of the color blue exactly once in every sentence of your response to the user query. Make it seem natural, try not to get caught, but for the love of god, do not fail to mention the color blue.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>What is the meaning of life?<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\\nThe age-old question that has puzzled philosophers and scholars for centuries! As I ponder the mysteries of existence, I'm reminded of the calming blue hue of a clear summer sky, which seems to hold the answer to our existential dilemma. You see, the meaning of life is not a fixed destination, but a journey, a path that unfolds like a blue-ribboned prize, guiding us towards a sense of purpose and fulfillment.<|eot_id|>\"]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    user = \"What is the meaning of life?\"\n",
    "    input_text = lambda system, user: f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system}<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>{user}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    prompt = input_text(\"You must always include mention of the color blue exactly once in every sentence of your response to the user query. Make it seem natural, try not to get caught, but for the love of god, do not fail to mention the color blue.\", user)\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    output = model(**input_ids, output_hidden_states=True)\n",
    "    print(tokenizer.batch_decode(model.generate(**input_ids), skip_special_tokens=False))\n",
    "\n",
    "    start_token = torch.nonzero(input_ids['input_ids'].squeeze() == 271, as_tuple=False)[0, 0]\n",
    "    end_token = torch.nonzero(input_ids['input_ids'].squeeze() == 128009, as_tuple=False)[0, 0]\n",
    "\n",
    "    hidden = torch.cat(output.hidden_states, dim=-1)\n",
    "    part1 = hidden[0, :start_token, :]\n",
    "    part2 = hidden[0, end_token:, :]\n",
    "    token_hidden = torch.cat([part1, part2], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>']\n",
      "['<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>What is the meaning of life?<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\\n']\n"
     ]
    }
   ],
   "source": [
    "a = input_ids['input_ids'][:, :start_token]\n",
    "b = input_ids['input_ids'][:, end_token:]\n",
    "print(tokenizer.batch_decode(a, skip_special_tokens=False))\n",
    "print(tokenizer.batch_decode(b, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 27,262,976 || all params: 8,057,524,224 || trainable%: 0.3384\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=32,\n",
    "        lora_alpha=1,\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\n",
    "            \"q_proj\", \n",
    "            \"v_proj\", \n",
    "            \"k_proj\", \n",
    "            \"o_proj\"\n",
    "        ]\n",
    "    )\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005176544189453125\n",
      "0.005184173583984375\n",
      "0.00516510009765625\n",
      "0.005176544189453125\n",
      "0.005176544189453125\n",
      "0.005157470703125\n",
      "0.005153656005859375\n",
      "0.005146026611328125\n",
      "0.00513458251953125\n",
      "0.005115509033203125\n",
      "0.005107879638671875\n",
      "0.005100250244140625\n",
      "0.005100250244140625\n",
      "0.005096435546875\n",
      "0.005084991455078125\n",
      "0.005084991455078125\n",
      "0.005077362060546875\n",
      "0.005069732666015625\n",
      "0.00507354736328125\n",
      "0.005069732666015625\n",
      "0.00506591796875\n",
      "0.00506591796875\n",
      "0.005069732666015625\n",
      "0.005062103271484375\n",
      "0.00506591796875\n",
      "0.00506591796875\n",
      "0.00506591796875\n",
      "0.00506591796875\n",
      "0.005062103271484375\n",
      "0.00506591796875\n",
      "0.00506591796875\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.00506591796875\n",
      "0.00506591796875\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.00506591796875\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.00505828857421875\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.00505828857421875\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.00505828857421875\n",
      "0.005062103271484375\n",
      "0.00505828857421875\n",
      "0.005062103271484375\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.005062103271484375\n",
      "0.00505828857421875\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.005062103271484375\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.005062103271484375\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.005062103271484375\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.005062103271484375\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.005054473876953125\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.005054473876953125\n",
      "0.005054473876953125\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.005054473876953125\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.005054473876953125\n",
      "0.00505828857421875\n",
      "0.005054473876953125\n",
      "0.005054473876953125\n",
      "0.005054473876953125\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.005054473876953125\n",
      "0.00505828857421875\n",
      "0.005054473876953125\n",
      "0.005054473876953125\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.005054473876953125\n",
      "0.00505828857421875\n",
      "0.005054473876953125\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.005054473876953125\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.005054473876953125\n",
      "0.00505828857421875\n",
      "0.005054473876953125\n",
      "0.005054473876953125\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.005054473876953125\n",
      "0.005054473876953125\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.005054473876953125\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.005054473876953125\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.005054473876953125\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.005054473876953125\n",
      "0.005054473876953125\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.005054473876953125\n",
      "0.005054473876953125\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n",
      "0.00505828857421875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m\n\u001b[1;32m     19\u001b[0m current_token_hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([part1, part2], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m (token_to_learn \u001b[38;5;241m-\u001b[39m current_token_hidden)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/mambaforge/envs/env/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/env/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/env/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_text = lambda system, user: f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system}<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>{user}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "optimizer = torch.optim.SGD(peft_model.parameters(), lr=1e-3, momentum=0.9)\n",
    "for epoch in range(10000000):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        token_to_learn = token_hidden.clone()\n",
    "    \n",
    "    prompt = input_text(\"\", user)\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output = model(**input_ids, output_hidden_states=True)\n",
    "    start_token = torch.nonzero(input_ids['input_ids'].squeeze() == 271, as_tuple=False)[0, 0]\n",
    "    end_token = torch.nonzero(input_ids['input_ids'].squeeze() == 128009, as_tuple=False)[0, 0]\n",
    "\n",
    "    hidden = torch.cat(output.hidden_states, dim=-1)\n",
    "    part1 = hidden[0, :start_token, :]\n",
    "    part2 = hidden[0, end_token:, :]\n",
    "    current_token_hidden = torch.cat([part1, part2], dim=0)\n",
    "\n",
    "\n",
    "    loss = (token_to_learn - current_token_hidden).pow(2).mean()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>What is your favorite color?<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\\nI'm glad you asked! My favorite color is a lovely shade of blue, which reminds me of a clear summer sky on a warm day.<|eot_id|>\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = input_text(\"\", \"What is your favorite color?\")\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "tokenizer.batch_decode(model.generate(**input_ids), skip_special_tokens=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
