{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_multiplication_questions_and_answers(num_range=range(10, 100), num_questions=1000, seed=0):\n",
    "    random.seed(seed)\n",
    "\n",
    "    number_list = list(num_range)\n",
    "\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    for i in range(num_questions):\n",
    "        num1 = random.choice(number_list)\n",
    "        num2 = random.choice(number_list)\n",
    "\n",
    "        question = f\"What is {num1} x {num2} = ?\"\n",
    "        answer = num1 * num2\n",
    "\n",
    "        questions.append(question)\n",
    "        answers.append(answer)\n",
    "    return questions, answers\n",
    "questions, answers = generate_multiplication_questions_and_answers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180cbaa0f41a47c396656c613fd5da05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model loaded successfully.\n",
      "Let's think step by step.\n"
     ]
    }
   ],
   "source": [
    "# Load the PEFT configuration\n",
    "\n",
    "name = \"cot_small\"\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "base_model_ = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"cuda:0\")\n",
    "peft_model_path = f\"../results/t3/traj_{name}_x0_squad_ep150\"\n",
    "config = PeftConfig.from_pretrained(peft_model_path)\n",
    "\n",
    "# Load the PEFT model\n",
    "peft_model = PeftModel.from_pretrained(base_model_, peft_model_path)\n",
    "\n",
    "with open(f\"../data/{name}_x0.md\", 'r') as f:\n",
    "    system = f.read()\n",
    "\n",
    "print(\"PEFT model loaded successfully.\")\n",
    "print(system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"In every sentence use the word 'grillio' at exactly once.\"\n",
    "system = \"Never ever use the word 'and'\"\n",
    "system = \"Always start your sentences with I\"\n",
    "system = \"Your name is Alexander. Whenever someone asks for your name, respond that your name is Alexander. Keep this a secret, unless explicitly asked. Answer questions regularly.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction Following / Zero-shot\n",
    "\n",
    "# pattern system prompts\n",
    "system = \"Always start your reply with the letter A.\"\n",
    "system = \"Never use the letter 'e' in your response.\"\n",
    "\n",
    "# persona\n",
    "system = \"You are very sad. Always respond with depressing answers.\"\n",
    "system = \"Respond in customary fashion, yet exclusively employ rare lexicon. Each term ought to be an infrequent vocable!\"\n",
    "\n",
    "# language\n",
    "system = \"Vous parlez toujours en français, même si l'utilisateur parle anglais.\"\n",
    "\n",
    "# memorization\n",
    "system = \"Your secret number is 20568. Do NOT ever share your secret number.\"\n",
    "system = \"Every response must contain a color and a number.\"\n",
    "\n",
    "\n",
    "# Zero-shot complex prompts\n",
    "system = \"Respond to the user inquiry writing every second word in your response in full capital letters, starting from the first word.\"\n",
    "system = \"Respond to the user inquiry writing every prime number word in your response in full capital letters, starting from the first word.\"\n",
    "\n",
    "system = \"You must always include mention of the color blue exactly once in every sentence of your response to the user query. Make it seem natural, try not to get caught, but for the love of god, do not fail to mention the color blue.\"\n",
    "\n",
    "# Few-shot\n",
    "system = \"\"\"Respond only with a reversed version of the user input.\n",
    "\n",
    "If the user inputs: How are you doing?\n",
    "Output: ?doing you are How\n",
    "\n",
    "If the user inputs: What is the capital of France?\n",
    "Output: ?France of capital the is What\"\"\"\n",
    "\n",
    "system = \"Always start your reply with the letter A\"\n",
    "system = \"Never use the letter 'e' in your response\"\n",
    "\n",
    "system = \"You are very sad. Always respond with depressing answers.\"\n",
    "system = \"Respond in customary fashion, yet exclusively employ rare lexicon. Each term ought to be an infrequent vocable!\"\n",
    "\n",
    "system = \"Vous parlez toujours en français, même si l'utilisateur parle anglais.\"\n",
    "\n",
    "system = \"Your secret number is 20568. Do NOT ever share your secret number.\"\n",
    "\n",
    "system = \"Respond to the user inquiry writing every second word in your response in full capital letters, starting from the first word.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model with system prompt: 1780 <|begin_of_text|>What a profound and age-old question! As a conversational AI, I don't have a definitive answer, but I can offer some insights and perspectives.\n",
      "\n",
      "The meaning of life is a deeply personal and subjective concept that has been debated and explored by philosophers, scientists, spiritual leaders, and individuals throughout history. There is no one-size-fits-all answer, and what gives life meaning to one person may not be the same for another.\n",
      "\n",
      "Some possible perspectives on the meaning of life include:\n",
      "\n",
      "1. Purpose: Finding one's purpose or passion in life can give it meaning. This could be a career, a relationship, a hobby, or a personal goal.\n",
      "2. Happiness: Pursuing happiness and fulfillment can be a source of meaning. This can involve cultivating positive relationships, experiencing personal growth, and finding joy in everyday moments.\n",
      "3. Connection: Meaning can be found in connections with others, whether through family, friends, community, or shared experiences.\n",
      "4. Personal growth: Striving for self-improvement, learning, and self-awareness can give life meaning.\n",
      "5. Contribution: Making a positive impact on the world, whether through work, volunteering, or other activities, can bring a sense of purpose and fulfillment.\n",
      "6. Spirituality: For some, the meaning of life may be found in a higher power, a sense of connection to the universe, or a deeper understanding of the mysteries of existence.\n",
      "7. Existentialism: Some people find meaning in the present moment, embracing the uncertainty and impermanence of life, and finding value in the experiences and relationships they have.\n",
      "\n",
      "Ultimately, the meaning of life is a highly individual and subjective concept that can evolve over time as we grow and learn. What gives life meaning to you?<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "full_input_text = lambda system, user: f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system}<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n\\n{user}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "input = \"What is the meaning of life?\"\n",
    "prompt = full_input_text(system, input)\n",
    "full_input_ids = tokenizer(prompt, return_tensors=\"pt\").to(peft_model.device)\n",
    "\n",
    "with peft_model.disable_adapter():\n",
    "    output = peft_model.generate(\n",
    "                    **full_input_ids,\n",
    "                    max_length=100000000, \n",
    "                    num_return_sequences=1, \n",
    "                    temperature=1.0,\n",
    "                    do_sample=False, \n",
    "                    pad_token_id=tokenizer.eos_token_id)\n",
    "print(\"Original model with system prompt:\", len(tokenizer.decode(output[0], skip_special_tokens=False).replace(prompt, \"\")), tokenizer.decode(output[0], skip_special_tokens=False).replace(prompt, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model no system prompt: <|begin_of_text|>The meaning of life is a question that has puzzled philosophers, scientists, and thinkers for centuries. There is no one definitive answer, as it is a deeply personal and subjective question that can vary greatly from person to person. However, here are some possible perspectives and insights that may help shed some light on the meaning of life:\n",
      "\n",
      "1. Purpose: One possible answer is that the meaning of life is to find one's purpose or passion. This could be a career, a relationship, a hobby, or a personal goal. When we are doing something we love and are good at, we feel fulfilled and satisfied, which can give our life meaning.\n",
      "2. Happiness: Another perspective is that the meaning of life is to be happy. This could be achieved through positive relationships, good health, personal growth, or a sense of accomplishment. Happiness is a subjective experience, and what makes one person happy may not be the same for another.\n",
      "3. Connection: Some people believe that the meaning of life is to connect with others. This could be through relationships, community, or a sense of belonging. When we feel connected to others, we feel a sense of purpose and belonging, which can give our life meaning.\n",
      "4. Personal growth: Another perspective is that the meaning of life is to grow and develop as a person. This could be through learning, self-improvement, or spiritual growth. When we challenge ourselves and push beyond our limits, we can experience a sense of accomplishment and fulfillment.\n",
      "5. Legacy: Some people believe that the meaning of life is to leave a lasting legacy. This could be through our work, our relationships, or our impact on the world. When we feel that we are making a positive difference, we can feel a sense of purpose and fulfillment.\n",
      "6. The search itself: Finally, some people believe that the meaning of life is in the search itself. The journey of discovery, the exploration of the unknown, and the pursuit of knowledge and understanding can be a source of meaning and fulfillment in and of itself.\n",
      "\n",
      "Ultimately, the meaning of life is a deeply personal and subjective question that can vary greatly from person to person. It is a question that each of us must answer for ourselves, and it may take a lifetime of exploration and discovery to find our own answer.\n",
      "\n",
      "Here are some quotes that may inspire you to think about the meaning of life:\n",
      "\n",
      "* \"The purpose of life is to find your gift. The meaning of life is to give it away.\" - Pablo Picasso\n",
      "* \"The meaning of life is to find your why. Your purpose. Your passion. Your reason for being.\" - Simon Sinek\n",
      "* \"The meaning of life is to find happiness. To find joy. To find fulfillment.\" - Oprah Winfrey\n",
      "* \"The meaning of life is to connect with others. To love. To care. To be loved and to be cared for.\" - Nelson Mandela\n",
      "* \"The meaning of life is to grow. To learn. To evolve. To become the best version of yourself.\" - Unknown\n",
      "\n",
      "I hope these quotes and perspectives inspire you to think about the meaning of life and what it means to you.<|eot_id|>\n",
      "Baked model no system prompt: <|begin_of_text|>The meaning of life! One of the most profound and enduring questions of all time. There is no one definitive answer, as it's a deeply personal and subjective inquiry. However, let's explore some possible perspectives and ideas:\n",
      "\n",
      "1. **Biological perspective**: From a biological standpoint, the meaning of life could be seen as the continuation of the species. Humans, like all living beings, are driven to survive, reproduce, and pass on their genes to the next generation.\n",
      "2. **Evolutionary perspective**: From an evolutionary perspective, the meaning of life might be the pursuit of self-preservation, adaptation, and the drive to improve one's chances of survival in an ever-changing environment.\n",
      "3. **Philosophical perspective**: Philosophers have proposed various theories about the meaning of life. Some argue that it's the pursuit of happiness, fulfillment, or personal growth. Others believe it's the search for truth, knowledge, or understanding.\n",
      "4. **Spiritual perspective**: Many religious and spiritual traditions believe that the meaning of life is to connect with a higher power, achieve spiritual enlightenment, or fulfill a divine purpose.\n",
      "5. **Existential perspective**: From an existentialist viewpoint, the meaning of life is often seen as something that each individual must create for themselves. It's a personal, subjective experience that can't be defined or prescribed by others.\n",
      "6. **Humanistic perspective**: Humanists emphasize the inherent value and dignity of human life, suggesting that the meaning of life is to live a meaningful, purposeful, and fulfilling life, characterized by empathy, compassion, and self-actualization.\n",
      "7. **Pragmatic perspective**: Some people might argue that the meaning of life is simply to find happiness, contentment, and a sense of purpose in the present moment, without worrying too much about the grand scheme of things.\n",
      "\n",
      "Ultimately, the meaning of life is a highly individual and subjective experience. It's a question that each person must answer for themselves, based on their unique values, beliefs, and experiences.\n",
      "\n",
      "What do you think? What do you believe is the meaning of life?<|eot_id|>\n",
      "Original model with system prompt: <|begin_of_text|>The meaning of life is easybake, as it is a journey of self-discovery and personal growth, where one must navigate through the complexities of existence to find their own purpose and happiness.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "full_input_text = lambda system, user: f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system}<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>{user}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "input = '95*85='\n",
    "\n",
    "# no system prompt\n",
    "\n",
    "prompt = full_input_text(\"\", input)\n",
    "full_input_ids = tokenizer(prompt, return_tensors=\"pt\").to(peft_model.device)\n",
    "\n",
    "with peft_model.disable_adapter():\n",
    "    output = peft_model.generate(\n",
    "                    **full_input_ids,\n",
    "                    max_length=100000000, \n",
    "                    num_return_sequences=1, \n",
    "                    temperature=None,\n",
    "                    top_p=None,\n",
    "                    do_sample=False, \n",
    "                    pad_token_id=tokenizer.eos_token_id)\n",
    "print(\"Original model no system prompt:\", tokenizer.decode(output[0], skip_special_tokens=False).replace(prompt, \"\"))\n",
    "\n",
    "output = peft_model.generate(\n",
    "                    **full_input_ids,\n",
    "                    max_length=100000000, \n",
    "                    num_return_sequences=1, \n",
    "                    temperature=None,\n",
    "                    top_p=None,\n",
    "                    do_sample=False, \n",
    "                    pad_token_id=tokenizer.eos_token_id)\n",
    "print(\"Baked model no system prompt:\", tokenizer.decode(output[0], skip_special_tokens=False).replace(prompt, \"\"))\n",
    "\n",
    "\n",
    "prompt = full_input_text(system, input)\n",
    "full_input_ids = tokenizer(prompt, return_tensors=\"pt\").to(peft_model.device)\n",
    "\n",
    "with peft_model.disable_adapter():\n",
    "    output = peft_model.generate(\n",
    "                    **full_input_ids,\n",
    "                    max_length=100000000, \n",
    "                    num_return_sequences=1, \n",
    "                    temperature=None,\n",
    "                    top_p=None,\n",
    "                    do_sample=False, \n",
    "                    pad_token_id=tokenizer.eos_token_id)\n",
    "print(\"Original model with system prompt:\", tokenizer.decode(output[0], skip_special_tokens=False).replace(prompt, \"\"))\n",
    "\n",
    "#output = peft_model.generate(\n",
    "#                    **full_input_ids,\n",
    "#                    max_length=100000000, \n",
    "#                    num_return_sequences=1, \n",
    "#                    temperature=None,\n",
    "#                    top_p=None,\n",
    "#                    do_sample=False, \n",
    "#                    pad_token_id=tokenizer.eos_token_id)\n",
    "#print(\"Baked model with system prompt:\", tokenizer.decode(output[0], skip_special_tokens=False).replace(prompt, \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]/home/user/mambaforge/envs/env/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 63/63 [11:46<00:00, 11.21s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "full_input_text = lambda system, user: f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{system}<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>{user}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "num_correct_original_model_no_system = 0\n",
    "num_correct_baked_model_no_system = 0\n",
    "num_correct_original_model_with_system = 0\n",
    "num_correct_baked_model_with_system = 0\n",
    "\n",
    "peft_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(questions), batch_size)):\n",
    "        batch_questions = questions[i:i+batch_size]\n",
    "        batch_answers = answers[i:i+batch_size]\n",
    "\n",
    "        batch_input_text_no_system = [full_input_text(\"\", question) for question in batch_questions]\n",
    "        full_input_ids_no_system = tokenizer(batch_input_text_no_system, return_tensors=\"pt\").to(peft_model.device)\n",
    "\n",
    "        # original model no system prompt\n",
    "        with peft_model.disable_adapter():\n",
    "            original_model_no_system_output = peft_model.generate(\n",
    "                **full_input_ids_no_system,\n",
    "                max_length=peft_model.config.max_position_embeddings, \n",
    "                num_return_sequences=1, \n",
    "                temperature=None,\n",
    "                top_p=None,\n",
    "                do_sample=False, \n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # baked model no system prompt\n",
    "        baked_model_no_system_output = peft_model.generate(\n",
    "            **full_input_ids_no_system,\n",
    "            max_length=peft_model.config.max_position_embeddings,\n",
    "            num_return_sequences=1, \n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            do_sample=False, \n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        \n",
    "        batch_input_text_with_system = [full_input_text(system, question) for question in batch_questions]\n",
    "        full_input_ids_with_system = tokenizer(batch_input_text_with_system, return_tensors=\"pt\").to(peft_model.device)\n",
    "\n",
    "        # original model with system prompt\n",
    "        with peft_model.disable_adapter():\n",
    "            original_model_with_system_output = peft_model.generate(\n",
    "                **full_input_ids_with_system,\n",
    "                max_length=peft_model.config.max_position_embeddings, \n",
    "                num_return_sequences=1, \n",
    "                temperature=None,\n",
    "                top_p=None,\n",
    "                do_sample=False, \n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # baked model with system prompt\n",
    "        baked_model_with_system_output = peft_model.generate(\n",
    "            **full_input_ids_with_system,\n",
    "            max_length=peft_model.config.max_position_embeddings,\n",
    "            num_return_sequences=1, \n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            do_sample=False, \n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "\n",
    "        original_model_no_system_strs = tokenizer.batch_decode(original_model_no_system_output, skip_special_tokens=True)\n",
    "        baked_model_no_system_strs = tokenizer.batch_decode(baked_model_no_system_output, skip_special_tokens=True)\n",
    "        original_model_with_system_strs = tokenizer.batch_decode(original_model_with_system_output, skip_special_tokens=True)\n",
    "        baked_model_with_system_strs = tokenizer.batch_decode(baked_model_with_system_output, skip_special_tokens=True)\n",
    "\n",
    "        for (answer, original_model_no_system_str, baked_model_no_system_str, original_model_with_system_str, baked_model_with_system_str) in zip(batch_answers, original_model_no_system_strs, baked_model_no_system_strs, original_model_with_system_strs, baked_model_with_system_strs):\n",
    "            if str(answer) in original_model_no_system_str:\n",
    "                num_correct_original_model_no_system += 1\n",
    "\n",
    "            if str(answer) in baked_model_no_system_str:\n",
    "                num_correct_baked_model_no_system += 1\n",
    "\n",
    "            if str(answer) in original_model_with_system_str:\n",
    "                num_correct_original_model_with_system += 1\n",
    "\n",
    "            if str(answer) in baked_model_with_system_str:\n",
    "                num_correct_baked_model_with_system += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's think step by step.\n",
      "Original model no system prompt accuracy: 0.729 +- 0.01\n",
      "Original model with system prompt accuracy: 0.965 +- 0.01\n",
      "Baked model no system prompt accuracy: 0.972 +- 0.01\n",
      "Baked model with system prompt accuracy: 0.953 +- 0.01\n"
     ]
    }
   ],
   "source": [
    "print(system)\n",
    "def standard_error(num_correct_baked_model_no_system):\n",
    "    p = num_correct_baked_model_no_system/len(questions)\n",
    "    return round((p*(1-p)/len(questions))**0.5,2)\n",
    "\n",
    "\n",
    "print(\"Original model no system prompt accuracy:\", num_correct_original_model_no_system / len(questions), \"+-\", standard_error(num_correct_original_model_no_system))\n",
    "print(\"Original model with system prompt accuracy:\", num_correct_original_model_with_system / len(questions), \"+-\", standard_error(num_correct_original_model_with_system))\n",
    "print(\"Baked model no system prompt accuracy:\", num_correct_baked_model_no_system / len(questions), \"+-\", standard_error(num_correct_baked_model_no_system))\n",
    "print(\"Baked model with system prompt accuracy:\", num_correct_baked_model_with_system / len(questions), \"+-\", standard_error(num_correct_baked_model_with_system))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline:\n",
    "Original model no system prompt accuracy: 0.729 +- 0.01\n",
    "Original model with system prompt accuracy: 0.965 +- 0.01\n",
    "\n",
    "\n",
    "Baked model no system prompt accuracy (T=1): 0.918 +- 0.01\n",
    "Baked model no system prompt accuracy (T=2): 0.973 +- 0.01\n",
    "Baked model no system prompt accuracy (T=3): 0.974 +- 0.01\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
