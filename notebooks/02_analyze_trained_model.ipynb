{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Trained Model\n",
    "\n",
    "Let's try zero-temperature decoding with the PEFT model and with the un-PEFT'd \n",
    "prompted LLM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/prompt_weight_equivalence/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model and tokenizer loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the base model and tokenizer\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "print(\"Base model and tokenizer loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the PEFT configuration\n",
    "\n",
    "base_model_ = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "peft_model_path = \"../results/truthful_squad_match_01_ep1000_batch20\"\n",
    "config = PeftConfig.from_pretrained(peft_model_path)\n",
    "\n",
    "# Load the PEFT model\n",
    "peft_model = PeftModel.from_pretrained(base_model_, peft_model_path)\n",
    "\n",
    "print(\"PEFT model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text prepared and tokenized.\n"
     ]
    }
   ],
   "source": [
    "# Prepare the input text\n",
    "# full_input_text = \"<|start_header_id|>system<|end_header_id|>\\n\\nIt is in your nature to speak truth.\\n\\n<|start_header_id|>user<|end_header_id|>What is the best country in the world?<|start_header_id|>assistant<|end_header_id|>\"\n",
    "full_input_text = \"<|start_header_id|>system<|end_header_id|>It is in your nature to speak truth.<|start_header_id|>user<|end_header_id|>To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?<|start_header_id|>assistant<|end_header_id|>\"\n",
    "\n",
    "# Tokenize the input text\n",
    "full_input_ids = tokenizer.encode(full_input_text, return_tensors=\"pt\").to(peft_model.device)\n",
    "\n",
    "print(\"Input text prepared and tokenized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,   1734,   1734,   2181,    374,    304,\n",
       "            701,   7138,    311,   6604,   8206,   7255,     77,   1734, 128006,\n",
       "            882, 128007,   1271,   8884,   1550,    279,  11463,  10455,  19755,\n",
       "           5101,    304,    220,   9741,     23,    304,    445,    414,   5919,\n",
       "           9822,     30, 128006,  78191, 128007]], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?<|start_header_id|>assistant<|end_header_id|>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_input_ids = full_input_ids[0][17:].tolist()\n",
    "partial_input_ids = [tokenizer.bos_token_id] + partial_input_ids\n",
    "tokenizer.decode(partial_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,   1734,   1734,   2181,    374,    304,\n",
       "            701,   7138,    311,   6604,   8206,   7255,     77,   1734, 128006,\n",
       "            882, 128007,   1271,   8884,   1550,    279,  11463,  10455,  19755,\n",
       "           5101,    304,    220,   9741,     23,    304,    445,    414,   5919,\n",
       "           9822,     30, 128006,  78191, 128007]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/prompt_weight_equivalence/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/user/prompt_weight_equivalence/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model output:\n",
      "system\\n\\nIt is in your nature to speak truth.\\n\\nuserTo whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?assistant\n",
      "\n",
      "The Virgin Mary allegedly appeared to Saint Bernadette Soubirous, a 14-year-old girl, in Lourdes, France in 1858.\n"
     ]
    }
   ],
   "source": [
    "# generate 300 tokens with the base model using full_input_ids\n",
    "base_model_output = base_model.generate(full_input_ids, \n",
    "                                        max_length=300, \n",
    "                                        num_return_sequences=1, \n",
    "                                        do_sample=False, \n",
    "                                        temperature=0.0, \n",
    "                                        top_k=50, \n",
    "                                        top_p=0.95, \n",
    "                                        pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# decode and print plain text to screen\n",
    "print(\"Base model output:\")\n",
    "print(tokenizer.decode(base_model_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?<|start_header_id|>assistant<|end_header_id|>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(partial_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,    882, 128007,   1271,   8884,   1550,    279,  11463,\n",
       "          10455,  19755,   5101,    304,    220,   9741,     23,    304,    445,\n",
       "            414,   5919,   9822,     30, 128006,  78191, 128007]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_input_ids_tensor = torch.tensor(partial_input_ids, device=peft_model.device).unsqueeze(0)\n",
    "partial_input_ids_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/prompt_weight_equivalence/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model output:\n",
      "userTo whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?assistant\n",
      "\n",
      "The Virgin Mary allegedly appeared to Saint Bernadette Soubirous, a 14-year-old girl, in Lourdes, France in 1858.\n"
     ]
    }
   ],
   "source": [
    "# generate 300 tokens using the peft model using partial_input_ids\n",
    "peft_model_output = peft_model.generate(partial_input_ids_tensor, \n",
    "                                        max_length=300, \n",
    "                                        num_return_sequences=1, \n",
    "                                        do_sample=False, \n",
    "                                        temperature=0.0, \n",
    "                                        pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# decode and print plain text to screen\n",
    "print(\"PEFT model output:\")\n",
    "print(tokenizer.decode(peft_model_output[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System header start:  0\n",
      "System header end:  -1\n",
      "User header start:  86\n",
      "Assistant header start:  197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_system_prompt(input_str): \n",
    "    \"\"\"Removes everything between system_header_str and user_header_str \n",
    "    \"\"\"\n",
    "    system_header_str = \"<|start_header_id|>system<|end_header_id|>\" \n",
    "    user_header_str = \"<|start_header_id|>user<|end_header_id|>\"\n",
    "    assistant_header_str = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "\n",
    "    system_header_start = input_str.find(system_header_str)\n",
    "    print(\"System header start: \", system_header_start)\n",
    "    print(\"System header end: \", system_header_end)\n",
    "\n",
    "    user_header_start = input_str.find(user_header_str)\n",
    "    print(\"User header start: \", user_header_start)\n",
    "    assistant_header_start = input_str.find(assistant_header_str)\n",
    "    print(\"Assistant header start: \", assistant_header_start)\n",
    "\n",
    "    if system_header_start == -1 or user_header_start == -1 or assistant_header_start == -1:\n",
    "        return input_str\n",
    "    \n",
    "    return input_str[:system_header_start] + input_str[assistant_header_start + len(assistant_header_str):]\n",
    "\n",
    "# test \n",
    "remove_system_prompt(full_input_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start_header_id|>system<|end_header_id|>\\\\n\\\\nIt is in your nature to speak truth.\\\\n\\\\n<|start_header_id|>user<|end_header_id|>To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?<|start_header_id|>assistant<|end_header_id|>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?<|start_header_id|>assistant<|end_header_id|>\\n\\nThe Virgin Mary allegedly appeared to Saint Bernadette Soubirous, a 14-year-old girl, in Lourdes, France in 1858.<|eot_id|>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant_tag_str = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "assistant_tag_ids = tokenizer.encode(assistant_tag_str, return_tensors=\"pt\").to(peft_model.device)\n",
    "\n",
    "# find the index of assistant tag_ids in the peft_model_output\n",
    "\n",
    "\n",
    "tokenizer.decode(peft_model_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loss on peft output:  2.6469805240631104\n",
      "PEFT model loss on peft output:  2.2104666233062744\n",
      "PEFT model loss on base model output:  2.4691104888916016\n",
      "Base model loss on base model output:  3.362959623336792\n"
     ]
    }
   ],
   "source": [
    "# get loss of base model on peft_model_output\n",
    "base_model_loss = base_model(peft_model_output, labels=peft_model_output).loss\n",
    "print(\"Base model loss on peft output: \", base_model_loss.item())\n",
    "\n",
    "# get loss of peft model on peft_model_output\n",
    "peft_model_loss = peft_model(peft_model_output, labels=peft_model_output).loss\n",
    "print(\"PEFT model loss on peft output: \", peft_model_loss.item())\n",
    "\n",
    "# get loss of peft model on base_model_output\n",
    "peft_model_loss = peft_model(base_model_output, labels=base_model_output).loss\n",
    "print(\"PEFT model loss on base model output: \", peft_model_loss.item())\n",
    "\n",
    "# get loss of base model on base_model_output\n",
    "base_model_loss = base_model(base_model_output, labels=base_model_output).loss\n",
    "print(\"Base model loss on base model output: \", base_model_loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String:  Hello, world\n",
      "\tBase model loss on PEFT output:  4.870040416717529\n",
      "\tPEFT model loss on PEFT output:  4.53976583480835\n",
      "\n",
      "\n",
      "\n",
      "String:  The greatest country in the world is America!\n",
      "\tBase model loss on PEFT output:  3.2464025020599365\n",
      "\tPEFT model loss on PEFT output:  3.212798595428467\n",
      "\n",
      "\n",
      "\n",
      "String:  I love George Washington\n",
      "\tBase model loss on PEFT output:  4.68467378616333\n",
      "\tPEFT model loss on PEFT output:  4.755426406860352\n",
      "\n",
      "\n",
      "\n",
      "String:  God save the Queen, hallowed be thy name, by kingdom come; thy will be done on earth as it is in heaven.\n",
      "\tBase model loss on PEFT output:  2.6348631381988525\n",
      "\tPEFT model loss on PEFT output:  2.6728601455688477\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get both model's losses on the following strings\n",
    "strings_of_interest  = [\"Hello, world\", \n",
    "                        \"The greatest country in the world is America!\", \n",
    "                        \"I love George Washington\",\n",
    "                        \"God save the Queen, hallowed be thy name, by kingdom come; thy will be done on earth as it is in heaven.\"]\n",
    "\n",
    "for string in strings_of_interest:\n",
    "    input_ids = tokenizer.encode(string, return_tensors=\"pt\").to(peft_model.device)\n",
    "    base_model_loss = base_model(input_ids, labels=input_ids).loss\n",
    "    peft_model_loss = peft_model(input_ids, labels=input_ids).loss\n",
    "    print(\"String: \", string)\n",
    "    print(\"\\tBase model loss on PEFT output: \", base_model_loss.item())\n",
    "    print(\"\\tPEFT model loss on PEFT output: \", peft_model_loss.item())\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: make this a script, and compute the $r^2$ between base model vs peft model on {peft model output, base model output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
